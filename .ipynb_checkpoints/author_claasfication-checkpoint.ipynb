{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "rcParams['figure.figsize'] = (16, 8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\", index_col = \"index\", encoding='utf-8')\n",
    "test = pd.read_csv(\"test_x.csv\", index_col = \"index\", encoding='utf-8')\n",
    "display(train.head(),test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = train[\"text\"][0]\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'was', 'almost', 'choking', '.', 'There', 'was', 'so', 'much', ',', 'so', 'much', 'he', 'wanted', 'to', 'say', ',', 'but', 'strange', 'exclamations', 'were', 'all', 'that', 'came', 'from', 'his', 'lips', '.', 'The', 'Pole', 'gazed', 'fixedly', 'at', 'him', ',', 'at', 'the', 'bundle', 'of', 'notes', 'in', 'his', 'hand', ';', 'looked', 'at', 'odin', ',', 'and', 'was', 'in', 'evident', 'perplexity', '.']\n"
     ]
    }
   ],
   "source": [
    "token = word_tokenize(t1)\n",
    "print(token)\n",
    "\n",
    "\n",
    "## word_tokenize() : 문장을 공백 단위로 구별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'wa',\n",
       " 'almost',\n",
       " 'choking',\n",
       " '.',\n",
       " 'There',\n",
       " 'wa',\n",
       " 'so',\n",
       " 'much',\n",
       " ',',\n",
       " 'so',\n",
       " 'much',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'say',\n",
       " ',',\n",
       " 'but',\n",
       " 'strange',\n",
       " 'exclamation',\n",
       " 'were',\n",
       " 'all',\n",
       " 'that',\n",
       " 'came',\n",
       " 'from',\n",
       " 'his',\n",
       " 'lip',\n",
       " '.',\n",
       " 'The',\n",
       " 'Pole',\n",
       " 'gazed',\n",
       " 'fixedly',\n",
       " 'at',\n",
       " 'him',\n",
       " ',',\n",
       " 'at',\n",
       " 'the',\n",
       " 'bundle',\n",
       " 'of',\n",
       " 'note',\n",
       " 'in',\n",
       " 'his',\n",
       " 'hand',\n",
       " ';',\n",
       " 'looked',\n",
       " 'at',\n",
       " 'odin',\n",
       " ',',\n",
       " 'and',\n",
       " 'wa',\n",
       " 'in',\n",
       " 'evident',\n",
       " 'perplexity',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "[lemmatizer.lemmatize(t) for t in token]\n",
    "\n",
    "## WordNetLemmatizer() : 단어의 원형으로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'was',\n",
       " 'almost',\n",
       " 'choke',\n",
       " '.',\n",
       " 'there',\n",
       " 'was',\n",
       " 'so',\n",
       " 'much',\n",
       " ',',\n",
       " 'so',\n",
       " 'much',\n",
       " 'he',\n",
       " 'want',\n",
       " 'to',\n",
       " 'say',\n",
       " ',',\n",
       " 'but',\n",
       " 'strang',\n",
       " 'exclam',\n",
       " 'were',\n",
       " 'all',\n",
       " 'that',\n",
       " 'came',\n",
       " 'from',\n",
       " 'his',\n",
       " 'lip',\n",
       " '.',\n",
       " 'the',\n",
       " 'pole',\n",
       " 'gaze',\n",
       " 'fix',\n",
       " 'at',\n",
       " 'him',\n",
       " ',',\n",
       " 'at',\n",
       " 'the',\n",
       " 'bundl',\n",
       " 'of',\n",
       " 'note',\n",
       " 'in',\n",
       " 'his',\n",
       " 'hand',\n",
       " ';',\n",
       " 'look',\n",
       " 'at',\n",
       " 'odin',\n",
       " ',',\n",
       " 'and',\n",
       " 'was',\n",
       " 'in',\n",
       " 'evid',\n",
       " 'perplex',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "[stemmer.stem(t) for t in token]\n",
    "\n",
    "## SnowballStemmer() : 단어에서 접사를 제거한 원형으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2683)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=word_tokenize, \n",
    "                      stop_words=stopwords.words('english'), \n",
    "                      ngram_range=(1, 2), \n",
    "                      min_df=100)\n",
    "\n",
    "X_cnt = vec.fit_transform(train['text'])\n",
    "print(X_cnt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cnt[0, :50].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 5899) (19617, 5899)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "X = vec.fit_transform(train['text'])\n",
    "X_tst = vec.transform(test['text'])\n",
    "print(X.shape, X_tst.shape)\n",
    "\n",
    "## TfidfVectorizer() : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, ..., 1, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y = train.author.values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros((X.shape[0], 5))\n",
    "p_tst = np.zeros((X_tst.shape[0], 5))\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X[i_trn], y[i_trn])\n",
    "    p[i_val, :] = clf.predict_proba(X[i_val])\n",
    "    p_tst += clf.predict_proba(X_tst) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (CV):  76.6687%\n",
      "Log Loss (CV):   0.6771\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p, axis=1)) * 100:8.4f}%')\n",
    "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p):8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19612</th>\n",
       "      <td>At the end of another day or two, odin growing...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>All afternoon we sat together, mostly in silen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19614</th>\n",
       "      <td>odin, having carried his thanks to odin, proc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19615</th>\n",
       "      <td>Soon after this, upon odin's leaving the room,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19616</th>\n",
       "      <td>And all the worse for the doomed man, that the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74496 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...     3.0\n",
       "1                 “Your sister asked for it, I suppose?”     2.0\n",
       "2       She was engaged one day as she walked, in per...     1.0\n",
       "3      The captain was in the porch, keeping himself ...     4.0\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...     3.0\n",
       "...                                                  ...     ...\n",
       "19612  At the end of another day or two, odin growing...     NaN\n",
       "19613  All afternoon we sat together, mostly in silen...     NaN\n",
       "19614   odin, having carried his thanks to odin, proc...     NaN\n",
       "19615  Soon after this, upon odin's leaving the room,...     NaN\n",
       "19616  And all the worse for the doomed man, that the...     NaN\n",
       "\n",
       "[74496 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train,test])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    15063\n",
       "0.0    13235\n",
       "2.0    11554\n",
       "4.0     7805\n",
       "1.0     7222\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"author\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 'in': 7,\n",
       " 'he': 8,\n",
       " 'was': 9,\n",
       " 'odin': 10,\n",
       " 'that': 11,\n",
       " 'it': 12,\n",
       " 'you': 13,\n",
       " '”': 14,\n",
       " 'his': 15,\n",
       " 'had': 16,\n",
       " 'with': 17,\n",
       " 'for': 18,\n",
       " 'as': 19,\n",
       " 'her': 20,\n",
       " 'at': 21,\n",
       " 'not': 22,\n",
       " 'my': 23,\n",
       " 'but': 24,\n",
       " 'is': 25,\n",
       " 'be': 26,\n",
       " 'have': 27,\n",
       " 'she': 28,\n",
       " 'me': 29,\n",
       " 'him': 30,\n",
       " 'on': 31,\n",
       " 'all': 32,\n",
       " 'said': 33,\n",
       " 'so': 34,\n",
       " 'this': 35,\n",
       " 'by': 36,\n",
       " 'from': 37,\n",
       " 'which': 38,\n",
       " 'were': 39,\n",
       " 'there': 40,\n",
       " 'one': 41,\n",
       " 'no': 42,\n",
       " 'they': 43,\n",
       " 'been': 44,\n",
       " 'would': 45,\n",
       " 'what': 46,\n",
       " 'we': 47,\n",
       " 'if': 48,\n",
       " 'an': 49,\n",
       " 'very': 50,\n",
       " 'are': 51,\n",
       " '’': 52,\n",
       " 'could': 53,\n",
       " 'when': 54,\n",
       " 'your': 55,\n",
       " 'out': 56,\n",
       " 'or': 57,\n",
       " 'will': 58,\n",
       " 'mr': 59,\n",
       " 'them': 60,\n",
       " 'up': 61,\n",
       " 'upon': 62,\n",
       " 'do': 63,\n",
       " 'more': 64,\n",
       " 'man': 65,\n",
       " 'who': 66,\n",
       " 'now': 67,\n",
       " 'some': 68,\n",
       " 'into': 69,\n",
       " 'their': 70,\n",
       " 'know': 71,\n",
       " 'am': 72,\n",
       " 'then': 73,\n",
       " 'time': 74,\n",
       " 'about': 75,\n",
       " 'only': 76,\n",
       " 'little': 77,\n",
       " 'like': 78,\n",
       " 'before': 79,\n",
       " 'see': 80,\n",
       " 'did': 81,\n",
       " 'should': 82,\n",
       " 'such': 83,\n",
       " 'than': 84,\n",
       " 'must': 85,\n",
       " 'any': 86,\n",
       " '“i': 87,\n",
       " \"'\": 88,\n",
       " 'come': 89,\n",
       " 'how': 90,\n",
       " 'down': 91,\n",
       " 'has': 92,\n",
       " 'much': 93,\n",
       " 'good': 94,\n",
       " 'our': 95,\n",
       " 'well': 96,\n",
       " 'say': 97,\n",
       " 'can': 98,\n",
       " 'never': 99,\n",
       " 'us': 100,\n",
       " 'again': 101,\n",
       " 'here': 102,\n",
       " 'too': 103,\n",
       " 'two': 104,\n",
       " 'think': 105,\n",
       " 'over': 106,\n",
       " 'other': 107,\n",
       " 'though': 108,\n",
       " 'own': 109,\n",
       " 'may': 110,\n",
       " 'made': 111,\n",
       " 'after': 112,\n",
       " 'himself': 113,\n",
       " 'great': 114,\n",
       " 'might': 115,\n",
       " 'old': 116,\n",
       " 'go': 117,\n",
       " 'way': 118,\n",
       " 'came': 119,\n",
       " 'first': 120,\n",
       " 'nothing': 121,\n",
       " 'day': 122,\n",
       " 'even': 123,\n",
       " 'last': 124,\n",
       " 'thought': 125,\n",
       " 'sir': 126,\n",
       " 'hand': 127,\n",
       " 'long': 128,\n",
       " 'back': 129,\n",
       " 'every': 130,\n",
       " 'house': 131,\n",
       " 'cried': 132,\n",
       " 'face': 133,\n",
       " 'away': 134,\n",
       " 'room': 135,\n",
       " 'mrs': 136,\n",
       " 'shall': 137,\n",
       " 'once': 138,\n",
       " 'without': 139,\n",
       " 'still': 140,\n",
       " 'being': 141,\n",
       " 'eyes': 142,\n",
       " 'most': 143,\n",
       " 'where': 144,\n",
       " 'these': 145,\n",
       " 'went': 146,\n",
       " 'looked': 147,\n",
       " 'make': 148,\n",
       " 'miss': 149,\n",
       " 'just': 150,\n",
       " 'myself': 151,\n",
       " 'young': 152,\n",
       " 'off': 153,\n",
       " 'yet': 154,\n",
       " 'something': 155,\n",
       " 'head': 156,\n",
       " 'tell': 157,\n",
       " 'take': 158,\n",
       " 'quite': 159,\n",
       " 'night': 160,\n",
       " 'ever': 161,\n",
       " 'life': 162,\n",
       " 'its': 163,\n",
       " 'same': 164,\n",
       " 'door': 165,\n",
       " 'saw': 166,\n",
       " 'look': 167,\n",
       " 'mind': 168,\n",
       " 'another': 169,\n",
       " 'always': 170,\n",
       " 'seemed': 171,\n",
       " 'left': 172,\n",
       " 'through': 173,\n",
       " 'heard': 174,\n",
       " 'why': 175,\n",
       " 'dear': 176,\n",
       " 'while': 177,\n",
       " 'three': 178,\n",
       " 'moment': 179,\n",
       " 'put': 180,\n",
       " 'took': 181,\n",
       " \"odin's\": 182,\n",
       " 'place': 183,\n",
       " 'give': 184,\n",
       " 'many': 185,\n",
       " 'thing': 186,\n",
       " 'heart': 187,\n",
       " 'asked': 188,\n",
       " 'don’t': 189,\n",
       " 'done': 190,\n",
       " 'better': 191,\n",
       " 'began': 192,\n",
       " 'right': 193,\n",
       " 'found': 194,\n",
       " 'going': 195,\n",
       " 'soon': 196,\n",
       " 'let': 197,\n",
       " 'looking': 198,\n",
       " 'get': 199,\n",
       " 'odin’s': 200,\n",
       " 'hands': 201,\n",
       " '“you': 202,\n",
       " 'prince': 203,\n",
       " 'father': 204,\n",
       " 'perhaps': 205,\n",
       " 'enough': 206,\n",
       " 'seen': 207,\n",
       " 'side': 208,\n",
       " 'knew': 209,\n",
       " 'herself': 210,\n",
       " 'round': 211,\n",
       " 'those': 212,\n",
       " 'told': 213,\n",
       " 'men': 214,\n",
       " 'people': 215,\n",
       " 'suddenly': 216,\n",
       " 'word': 217,\n",
       " 'got': 218,\n",
       " 'half': 219,\n",
       " 'lady': 220,\n",
       " 'course': 221,\n",
       " 'turned': 222,\n",
       " 'love': 223,\n",
       " 'under': 224,\n",
       " 'indeed': 225,\n",
       " 'believe': 226,\n",
       " 'however': 227,\n",
       " 'whole': 228,\n",
       " 'morning': 229,\n",
       " 'far': 230,\n",
       " 'woman': 231,\n",
       " 'friend': 232,\n",
       " 'almost': 233,\n",
       " 'both': 234,\n",
       " '“and': 235,\n",
       " 'sure': 236,\n",
       " 'anything': 237,\n",
       " 'felt': 238,\n",
       " 'home': 239,\n",
       " 'having': 240,\n",
       " 'find': 241,\n",
       " 'against': 242,\n",
       " 'sat': 243,\n",
       " 'mother': 244,\n",
       " 'stood': 245,\n",
       " 'words': 246,\n",
       " 'whom': 247,\n",
       " 'name': 248,\n",
       " 'end': 249,\n",
       " 'voice': 250,\n",
       " 'rather': 251,\n",
       " 'between': 252,\n",
       " 'years': 253,\n",
       " 'returned': 254,\n",
       " 'because': 255,\n",
       " 'want': 256,\n",
       " 'gone': 257,\n",
       " 'really': 258,\n",
       " 'poor': 259,\n",
       " 'replied': 260,\n",
       " 'few': 261,\n",
       " 'part': 262,\n",
       " 'hear': 263,\n",
       " 'together': 264,\n",
       " 'money': 265,\n",
       " 'among': 266,\n",
       " 'brought': 267,\n",
       " 'set': 268,\n",
       " 'it’s': 269,\n",
       " 'matter': 270,\n",
       " 'speak': 271,\n",
       " 'light': 272,\n",
       " 'gave': 273,\n",
       " 'understand': 274,\n",
       " 'since': 275,\n",
       " 'cannot': 276,\n",
       " 'open': 277,\n",
       " 'hope': 278,\n",
       " 'case': 279,\n",
       " 'yourself': 280,\n",
       " 'world': 281,\n",
       " 'things': 282,\n",
       " 'new': 283,\n",
       " 'table': 284,\n",
       " 'taken': 285,\n",
       " 'already': 286,\n",
       " '“but': 287,\n",
       " 'evening': 288,\n",
       " 'lay': 289,\n",
       " 'passed': 290,\n",
       " 'next': 291,\n",
       " 'hour': 292,\n",
       " '“it': 293,\n",
       " 'brother': 294,\n",
       " 'days': 295,\n",
       " 'least': 296,\n",
       " 'leave': 297,\n",
       " 'each': 298,\n",
       " 'family': 299,\n",
       " 'letter': 300,\n",
       " 'business': 301,\n",
       " 'boy': 302,\n",
       " 'until': 303,\n",
       " 'behind': 304,\n",
       " 'everything': 305,\n",
       " 'towards': 306,\n",
       " 'doctor': 307,\n",
       " 'ask': 308,\n",
       " 'general': 309,\n",
       " 'nor': 310,\n",
       " 'whether': 311,\n",
       " 'gentleman': 312,\n",
       " 'sister': 313,\n",
       " 'does': 314,\n",
       " '‘i': 315,\n",
       " 'answered': 316,\n",
       " 'best': 317,\n",
       " 'point': 318,\n",
       " 'air': 319,\n",
       " 'doubt': 320,\n",
       " 'coming': 321,\n",
       " 'wife': 322,\n",
       " 'wish': 323,\n",
       " 'alone': 324,\n",
       " 'small': 325,\n",
       " 'sort': 326,\n",
       " 'strange': 327,\n",
       " '“what': 328,\n",
       " 'idea': 329,\n",
       " 'mean': 330,\n",
       " 'question': 331,\n",
       " \"don't\": 332,\n",
       " 'lord': 333,\n",
       " 'window': 334,\n",
       " 'work': 335,\n",
       " 'manner': 336,\n",
       " 'god': 337,\n",
       " 'till': 338,\n",
       " 'help': 339,\n",
       " 'keep': 340,\n",
       " 'else': 341,\n",
       " 'kind': 342,\n",
       " 'remember': 343,\n",
       " 'present': 344,\n",
       " 'black': 345,\n",
       " 'ye': 346,\n",
       " 'oh': 347,\n",
       " 'town': 348,\n",
       " 'street': 349,\n",
       " 'fire': 350,\n",
       " 'short': 351,\n",
       " 'five': 352,\n",
       " 'read': 353,\n",
       " 'less': 354,\n",
       " 'called': 355,\n",
       " 'certainly': 356,\n",
       " 'answer': 357,\n",
       " 'yes': 358,\n",
       " 'known': 359,\n",
       " 'within': 360,\n",
       " 'true': 361,\n",
       " 'fact': 362,\n",
       " 'master': 363,\n",
       " 'often': 364,\n",
       " 'full': 365,\n",
       " '“well': 366,\n",
       " 'happy': 367,\n",
       " 'feel': 368,\n",
       " 'that’s': 369,\n",
       " 'walked': 370,\n",
       " 'added': 371,\n",
       " 'suppose': 372,\n",
       " 'reason': 373,\n",
       " 'given': 374,\n",
       " 'either': 375,\n",
       " 'fellow': 376,\n",
       " 'dark': 377,\n",
       " 'girl': 378,\n",
       " 'near': 379,\n",
       " 'used': 380,\n",
       " 'certain': 381,\n",
       " 'talk': 382,\n",
       " 'spoke': 383,\n",
       " 'child': 384,\n",
       " 'kept': 385,\n",
       " 'four': 386,\n",
       " 'high': 387,\n",
       " 'country': 388,\n",
       " 'ill': 389,\n",
       " 'rest': 390,\n",
       " 'afraid': 391,\n",
       " 'fell': 392,\n",
       " 'others': 393,\n",
       " 'ready': 394,\n",
       " 'possible': 395,\n",
       " '“yes': 396,\n",
       " 'hardly': 397,\n",
       " 'bed': 398,\n",
       " 'ran': 399,\n",
       " 'friends': 400,\n",
       " 'dead': 401,\n",
       " 'death': 402,\n",
       " 'aunt': 403,\n",
       " 'times': 404,\n",
       " '“the': 405,\n",
       " 'says': 406,\n",
       " 'white': 407,\n",
       " 'sometimes': 408,\n",
       " 'person': 409,\n",
       " 'call': 410,\n",
       " 'chair': 411,\n",
       " 'appeared': 412,\n",
       " \"it's\": 413,\n",
       " '“no': 414,\n",
       " 'clear': 415,\n",
       " 'captain': 416,\n",
       " 'afterwards': 417,\n",
       " 'struck': 418,\n",
       " 'along': 419,\n",
       " '“oh': 420,\n",
       " 'strong': 421,\n",
       " 'also': 422,\n",
       " 'observed': 423,\n",
       " 'lost': 424,\n",
       " 'feet': 425,\n",
       " 'turn': 426,\n",
       " '“that': 427,\n",
       " 'held': 428,\n",
       " 'nature': 429,\n",
       " 'feeling': 430,\n",
       " 'large': 431,\n",
       " 'making': 432,\n",
       " 'second': 433,\n",
       " 'sitting': 434,\n",
       " 'subject': 435,\n",
       " 'glad': 436,\n",
       " 'taking': 437,\n",
       " 'means': 438,\n",
       " 'smile': 439,\n",
       " 'deal': 440,\n",
       " 'water': 441,\n",
       " 'state': 442,\n",
       " 'pleasure': 443,\n",
       " 'followed': 444,\n",
       " 'hundred': 445,\n",
       " 'hard': 446,\n",
       " 'silence': 447,\n",
       " 'thousand': 448,\n",
       " 'above': 449,\n",
       " 'able': 450,\n",
       " 'mine': 451,\n",
       " 'opened': 452,\n",
       " 'eye': 453,\n",
       " 'close': 454,\n",
       " 'ten': 455,\n",
       " 'doing': 456,\n",
       " 'wanted': 457,\n",
       " 'truth': 458,\n",
       " 'met': 459,\n",
       " 'corner': 460,\n",
       " 'bring': 461,\n",
       " 'account': 462,\n",
       " 'need': 463,\n",
       " 'fear': 464,\n",
       " 'london': 465,\n",
       " 'forward': 466,\n",
       " 'continued': 467,\n",
       " 'company': 468,\n",
       " 'across': 469,\n",
       " 'body': 470,\n",
       " 'care': 471,\n",
       " 'interest': 472,\n",
       " 'minutes': 473,\n",
       " 'cold': 474,\n",
       " 'sent': 475,\n",
       " 'arms': 476,\n",
       " 'whose': 477,\n",
       " 'thinking': 478,\n",
       " 'pretty': 479,\n",
       " 'opinion': 480,\n",
       " '“he': 481,\n",
       " 'feelings': 482,\n",
       " 'walk': 483,\n",
       " 'children': 484,\n",
       " 'red': 485,\n",
       " 'past': 486,\n",
       " 'hold': 487,\n",
       " 'i’ll': 488,\n",
       " 'return': 489,\n",
       " 'show': 490,\n",
       " 'late': 491,\n",
       " 'sight': 492,\n",
       " 'became': 493,\n",
       " 'run': 494,\n",
       " 'low': 495,\n",
       " 'son': 496,\n",
       " 'happened': 497,\n",
       " 'seeing': 498,\n",
       " 'saying': 499,\n",
       " 'standing': 500,\n",
       " 'ought': 501,\n",
       " 'twenty': 502,\n",
       " 'arm': 503,\n",
       " 'use': 504,\n",
       " 'blood': 505,\n",
       " 'therefore': 506,\n",
       " 'character': 507,\n",
       " 'ground': 508,\n",
       " 'attention': 509,\n",
       " 'themselves': 510,\n",
       " 'story': 511,\n",
       " 'instant': 512,\n",
       " 'change': 513,\n",
       " 'beyond': 514,\n",
       " 'road': 515,\n",
       " 'ago': 516,\n",
       " 'sound': 517,\n",
       " 'silent': 518,\n",
       " 'gentlemen': 519,\n",
       " 'immediately': 520,\n",
       " 'bad': 521,\n",
       " 'turning': 522,\n",
       " 'talking': 523,\n",
       " 'hair': 524,\n",
       " 'different': 525,\n",
       " 'object': 526,\n",
       " 'appearance': 527,\n",
       " 'live': 528,\n",
       " 'party': 529,\n",
       " 'received': 530,\n",
       " 'tried': 531,\n",
       " 'sit': 532,\n",
       " 'conversation': 533,\n",
       " 'simply': 534,\n",
       " 'year': 535,\n",
       " 'ivan': 536,\n",
       " 'paper': 537,\n",
       " 'soul': 538,\n",
       " 'minute': 539,\n",
       " 'carried': 540,\n",
       " 'hours': 541,\n",
       " 'expression': 542,\n",
       " 'laughed': 543,\n",
       " 'several': 544,\n",
       " 'sea': 545,\n",
       " 'knows': 546,\n",
       " 'visit': 547,\n",
       " 'entered': 548,\n",
       " 'dare': 549,\n",
       " 'stopped': 550,\n",
       " 'husband': 551,\n",
       " 'bear': 552,\n",
       " 'sense': 553,\n",
       " 'six': 554,\n",
       " 'odins': 555,\n",
       " 'stand': 556,\n",
       " 'secret': 557,\n",
       " 'longer': 558,\n",
       " 'neither': 559,\n",
       " 'surprise': 560,\n",
       " 'none': 561,\n",
       " 'besides': 562,\n",
       " 'daughter': 563,\n",
       " 'fine': 564,\n",
       " 'deep': 565,\n",
       " 'power': 566,\n",
       " \"that's\": 567,\n",
       " 'sudden': 568,\n",
       " 'tears': 569,\n",
       " '“if': 570,\n",
       " '“why': 571,\n",
       " '“my': 572,\n",
       " 'joe': 573,\n",
       " 'led': 574,\n",
       " 'front': 575,\n",
       " 'creature': 576,\n",
       " 'waiting': 577,\n",
       " 'purpose': 578,\n",
       " 'quiet': 579,\n",
       " 'carriage': 580,\n",
       " 'lips': 581,\n",
       " 'speaking': 582,\n",
       " 'laughing': 583,\n",
       " 'drew': 584,\n",
       " 'wonder': 585,\n",
       " 'married': 586,\n",
       " 'impossible': 587,\n",
       " \"'i\": 588,\n",
       " 'laid': 589,\n",
       " 'won’t': 590,\n",
       " 'honour': 591,\n",
       " 'note': 592,\n",
       " 'beside': 593,\n",
       " 'during': 594,\n",
       " 'order': 595,\n",
       " 'forth': 596,\n",
       " 'sorry': 597,\n",
       " 'early': 598,\n",
       " 'dinner': 599,\n",
       " 'likely': 600,\n",
       " 'pass': 601,\n",
       " 'i’ve': 602,\n",
       " 'usual': 603,\n",
       " 'garden': 604,\n",
       " 'foot': 605,\n",
       " 'can’t': 606,\n",
       " 'repeated': 607,\n",
       " 'itself': 608,\n",
       " 'position': 609,\n",
       " 'fancy': 610,\n",
       " 'common': 611,\n",
       " 'become': 612,\n",
       " 'meant': 613,\n",
       " 'reached': 614,\n",
       " 'lying': 615,\n",
       " 'perfectly': 616,\n",
       " 'wall': 617,\n",
       " 'happiness': 618,\n",
       " 'remained': 619,\n",
       " 'yesterday': 620,\n",
       " 'stay': 621,\n",
       " 'wind': 622,\n",
       " 'later': 623,\n",
       " 'i’m': 624,\n",
       " 'week': 625,\n",
       " 'thoughts': 626,\n",
       " 'glass': 627,\n",
       " 'talked': 628,\n",
       " 'sake': 629,\n",
       " 'please': 630,\n",
       " 'straight': 631,\n",
       " 'spite': 632,\n",
       " 'cut': 633,\n",
       " 'heavy': 634,\n",
       " 'meet': 635,\n",
       " '“there': 636,\n",
       " 'expected': 637,\n",
       " 'seems': 638,\n",
       " 'didn’t': 639,\n",
       " 'seem': 640,\n",
       " 'showed': 641,\n",
       " 'ladies': 642,\n",
       " 'society': 643,\n",
       " 'lived': 644,\n",
       " 'book': 645,\n",
       " 'cry': 646,\n",
       " '“odin': 647,\n",
       " 'spirits': 648,\n",
       " 'wrong': 649,\n",
       " 'exactly': 650,\n",
       " 'raised': 651,\n",
       " 'broken': 652,\n",
       " 'angry': 653,\n",
       " 'step': 654,\n",
       " 'living': 655,\n",
       " 'acquaintance': 656,\n",
       " 'caught': 657,\n",
       " 'write': 658,\n",
       " 'save': 659,\n",
       " 'comes': 660,\n",
       " 'women': 661,\n",
       " 'chance': 662,\n",
       " 'broke': 663,\n",
       " 'floor': 664,\n",
       " 'jane': 665,\n",
       " 'occasion': 666,\n",
       " 'loved': 667,\n",
       " 'earth': 668,\n",
       " 'entirely': 669,\n",
       " 'wait': 670,\n",
       " 'spirit': 671,\n",
       " 'thus': 672,\n",
       " 'uncle': 673,\n",
       " 'walking': 674,\n",
       " 'real': 675,\n",
       " 'morrow': 676,\n",
       " 'line': 677,\n",
       " 'danger': 678,\n",
       " '“we': 679,\n",
       " 'getting': 680,\n",
       " '‘you': 681,\n",
       " 'pleased': 682,\n",
       " 'laugh': 683,\n",
       " 'wished': 684,\n",
       " 'surprised': 685,\n",
       " 'figure': 686,\n",
       " 'companion': 687,\n",
       " 'natural': 688,\n",
       " 'whatever': 689,\n",
       " 'particular': 690,\n",
       " 'steps': 691,\n",
       " 'public': 692,\n",
       " 'police': 693,\n",
       " '“how': 694,\n",
       " 'worse': 695,\n",
       " 'fall': 696,\n",
       " 'trouble': 697,\n",
       " 'beginning': 698,\n",
       " 'imagine': 699,\n",
       " 'service': 700,\n",
       " 'assure': 701,\n",
       " 'probably': 702,\n",
       " 'begin': 703,\n",
       " 'sleep': 704,\n",
       " 'marriage': 705,\n",
       " 'fair': 706,\n",
       " 'giving': 707,\n",
       " 'view': 708,\n",
       " 'months': 709,\n",
       " 'worth': 710,\n",
       " 'wine': 711,\n",
       " 'slowly': 712,\n",
       " 'pale': 713,\n",
       " 'horse': 714,\n",
       " 'respect': 715,\n",
       " 'cause': 716,\n",
       " 'stone': 717,\n",
       " 'knowledge': 718,\n",
       " 'running': 719,\n",
       " 'circumstances': 720,\n",
       " 'except': 721,\n",
       " 'seven': 722,\n",
       " 'self': 723,\n",
       " \"i'll\": 724,\n",
       " 'further': 725,\n",
       " 'box': 726,\n",
       " 'fortune': 727,\n",
       " 'latter': 728,\n",
       " 'stepan': 729,\n",
       " 'there’s': 730,\n",
       " 'dr': 731,\n",
       " 'notice': 732,\n",
       " 'anxious': 733,\n",
       " 'hat': 734,\n",
       " 'drawing': 735,\n",
       " 'court': 736,\n",
       " 'easy': 737,\n",
       " 'written': 738,\n",
       " 'die': 739,\n",
       " 'particularly': 740,\n",
       " 'moved': 741,\n",
       " 'shoulder': 742,\n",
       " 'church': 743,\n",
       " 'length': 744,\n",
       " 'shook': 745,\n",
       " 'especially': 746,\n",
       " 'settled': 747,\n",
       " 'although': 748,\n",
       " 'mouth': 749,\n",
       " 'died': 750,\n",
       " 'try': 751,\n",
       " 'hall': 752,\n",
       " 'instead': 753,\n",
       " 'necessary': 754,\n",
       " 'distance': 755,\n",
       " '“this': 756,\n",
       " 'roubles': 757,\n",
       " '‘and': 758,\n",
       " 'play': 759,\n",
       " 'extraordinary': 760,\n",
       " 'comfort': 761,\n",
       " 'murder': 762,\n",
       " 'believed': 763,\n",
       " 'anyone': 764,\n",
       " 'listened': 765,\n",
       " 'age': 766,\n",
       " 'follow': 767,\n",
       " 'watch': 768,\n",
       " 'send': 769,\n",
       " 'scarcely': 770,\n",
       " 'coat': 771,\n",
       " '“a': 772,\n",
       " 'effect': 773,\n",
       " 'tea': 774,\n",
       " \"i've\": 775,\n",
       " 'marry': 776,\n",
       " 'plain': 777,\n",
       " 'nearly': 778,\n",
       " 'passage': 779,\n",
       " 'forgive': 780,\n",
       " 'directly': 781,\n",
       " 'madame': 782,\n",
       " 'listen': 783,\n",
       " 'top': 784,\n",
       " 'obliged': 785,\n",
       " 'ha': 786,\n",
       " 'forgotten': 787,\n",
       " 'beautiful': 788,\n",
       " 'lie': 789,\n",
       " 'direction': 790,\n",
       " 'anne': 791,\n",
       " 'tone': 792,\n",
       " 'changed': 793,\n",
       " 'sun': 794,\n",
       " 'cross': 795,\n",
       " 'office': 796,\n",
       " 'instantly': 797,\n",
       " 'news': 798,\n",
       " 'aware': 799,\n",
       " 'situation': 800,\n",
       " 'piece': 801,\n",
       " 'arrived': 802,\n",
       " 'terrible': 803,\n",
       " 'allow': 804,\n",
       " 'consider': 805,\n",
       " 'drink': 806,\n",
       " 'free': 807,\n",
       " 'pay': 808,\n",
       " 'determined': 809,\n",
       " 'gate': 810,\n",
       " 'leaving': 811,\n",
       " 'trust': 812,\n",
       " 'fodin': 813,\n",
       " 'threw': 814,\n",
       " 'serious': 815,\n",
       " 'scene': 816,\n",
       " 'third': 817,\n",
       " 'questions': 818,\n",
       " 'future': 819,\n",
       " 'closed': 820,\n",
       " 'forget': 821,\n",
       " 'greater': 822,\n",
       " 'trying': 823,\n",
       " 'quickly': 824,\n",
       " 'presence': 825,\n",
       " 'engaged': 826,\n",
       " 'wood': 827,\n",
       " 'fixed': 828,\n",
       " 'convinced': 829,\n",
       " 'duty': 830,\n",
       " 'pleasant': 831,\n",
       " 'twice': 832,\n",
       " 'green': 833,\n",
       " 'knowing': 834,\n",
       " 'yours': 835,\n",
       " 'curiosity': 836,\n",
       " 'papers': 837,\n",
       " 'human': 838,\n",
       " 'clothes': 839,\n",
       " 'law': 840,\n",
       " 'dress': 841,\n",
       " 'stairs': 842,\n",
       " 'evil': 843,\n",
       " 'rooms': 844,\n",
       " 'burst': 845,\n",
       " 'wild': 846,\n",
       " 'heaven': 847,\n",
       " 'understood': 848,\n",
       " 'bit': 849,\n",
       " 'dora': 850,\n",
       " 'thank': 851,\n",
       " 'letters': 852,\n",
       " 'former': 853,\n",
       " 'supposed': 854,\n",
       " 'bright': 855,\n",
       " 'breath': 856,\n",
       " 'ashamed': 857,\n",
       " 'evidently': 858,\n",
       " 'spoken': 859,\n",
       " 'windows': 860,\n",
       " 'servant': 861,\n",
       " 'seat': 862,\n",
       " 'strength': 863,\n",
       " 'expect': 864,\n",
       " '“do': 865,\n",
       " 'desire': 866,\n",
       " 'faces': 867,\n",
       " 'meeting': 868,\n",
       " 'dropped': 869,\n",
       " 'remembered': 870,\n",
       " 'outside': 871,\n",
       " 'dog': 872,\n",
       " 'spent': 873,\n",
       " 'confidence': 874,\n",
       " 'england': 875,\n",
       " 'english': 876,\n",
       " 'looks': 877,\n",
       " 'single': 878,\n",
       " 'dreadful': 879,\n",
       " 'fit': 880,\n",
       " 'farther': 881,\n",
       " 'grew': 882,\n",
       " 'bound': 883,\n",
       " 'blue': 884,\n",
       " 'shut': 885,\n",
       " 'train': 886,\n",
       " 'sign': 887,\n",
       " 'pity': 888,\n",
       " 'fresh': 889,\n",
       " 'altogether': 890,\n",
       " 'nobody': 891,\n",
       " 'silver': 892,\n",
       " '“then': 893,\n",
       " 'affection': 894,\n",
       " '“ah': 895,\n",
       " 'judge': 896,\n",
       " 'spot': 897,\n",
       " 'occurred': 898,\n",
       " 'safe': 899,\n",
       " 'dressed': 900,\n",
       " 'glance': 901,\n",
       " 'shot': 902,\n",
       " 'smiling': 903,\n",
       " 'appear': 904,\n",
       " 'reply': 905,\n",
       " 'evidence': 906,\n",
       " 'beg': 907,\n",
       " 'quick': 908,\n",
       " 'influence': 909,\n",
       " 'rushed': 910,\n",
       " 'form': 911,\n",
       " 'reading': 912,\n",
       " \"i'm\": 913,\n",
       " 'remarked': 914,\n",
       " 'pride': 915,\n",
       " 'somewhat': 916,\n",
       " 'putting': 917,\n",
       " 'carry': 918,\n",
       " 'excellent': 919,\n",
       " 'terror': 920,\n",
       " 'smiled': 921,\n",
       " 'agreeable': 922,\n",
       " 'frightened': 923,\n",
       " 'difficult': 924,\n",
       " 'pain': 925,\n",
       " 'drawn': 926,\n",
       " 'quarter': 927,\n",
       " 'candle': 928,\n",
       " 'contrary': 929,\n",
       " 'complete': 930,\n",
       " 'ears': 931,\n",
       " 'ourselves': 932,\n",
       " 'eight': 933,\n",
       " 'journey': 934,\n",
       " 'horses': 935,\n",
       " 'beauty': 936,\n",
       " 'important': 937,\n",
       " 'paid': 938,\n",
       " 'breakfast': 939,\n",
       " 'pray': 940,\n",
       " 'absolutely': 941,\n",
       " 'exclaimed': 942,\n",
       " 'started': 943,\n",
       " 'trees': 944,\n",
       " 'river': 945,\n",
       " 'horror': 946,\n",
       " 'tall': 947,\n",
       " 'easily': 948,\n",
       " 'prisoner': 949,\n",
       " 'he’s': 950,\n",
       " 'whispered': 951,\n",
       " 'grave': 952,\n",
       " 'act': 953,\n",
       " 'begun': 954,\n",
       " 'boys': 955,\n",
       " 'number': 956,\n",
       " 'fond': 957,\n",
       " 'thrown': 958,\n",
       " 'passing': 959,\n",
       " 'crowd': 960,\n",
       " 'fool': 961,\n",
       " 'satisfaction': 962,\n",
       " 'considered': 963,\n",
       " 'mad': 964,\n",
       " 'surely': 965,\n",
       " 'private': 966,\n",
       " 'countenance': 967,\n",
       " 'mere': 968,\n",
       " 'stop': 969,\n",
       " 'delighted': 970,\n",
       " 'ship': 971,\n",
       " 'force': 972,\n",
       " 'darkness': 973,\n",
       " 'bell': 974,\n",
       " 'impression': 975,\n",
       " 'promise': 976,\n",
       " 'extremely': 977,\n",
       " 'touch': 978,\n",
       " 'land': 979,\n",
       " 'shaking': 980,\n",
       " 'proud': 981,\n",
       " 'waited': 982,\n",
       " 'joy': 983,\n",
       " 'sofa': 984,\n",
       " '“it’s': 985,\n",
       " 'wrote': 986,\n",
       " 'meaning': 987,\n",
       " 'kindness': 988,\n",
       " 'mention': 989,\n",
       " 'facts': 990,\n",
       " 'pulled': 991,\n",
       " 'speech': 992,\n",
       " 'rich': 993,\n",
       " 'miles': 994,\n",
       " 'handsome': 995,\n",
       " '“is': 996,\n",
       " 'difficulty': 997,\n",
       " 'excuse': 998,\n",
       " 'hot': 999,\n",
       " 'devil': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 마이닝\n",
    "\n",
    "# 1. 단어 등록\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(df[\"text\"])\n",
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52997"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. text 맵핑\n",
    "all_text = tk.texts_to_sequences(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    7, 1332, 3045],\n",
       "       [   0,    0,    0, ...,    6,  372,   14],\n",
       "       [   0,    0,    0, ...,  439,   28,   33],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    7,   20,  162],\n",
       "       [   0,    0,    0, ...,   30,   21, 4834],\n",
       "       [   0,    0,    0, ...,    4,  838, 1863]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. padding하기\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_sequence = pad_sequences(all_text)\n",
    "pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequence[:len(train)]\n",
    "test_pad = pad_sequence[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "193/193 [==============================] - ETA: 0s - loss: 1.4216 - acc: 0.3967\n",
      "Epoch 00001: val_loss improved from inf to 1.13155, saving model to best.h5\n",
      "193/193 [==============================] - 121s 628ms/step - loss: 1.4216 - acc: 0.3967 - val_loss: 1.1315 - val_acc: 0.5712\n",
      "Epoch 2/10\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.9801 - acc: 0.6282\n",
      "Epoch 00002: val_loss improved from 1.13155 to 0.92700, saving model to best.h5\n",
      "193/193 [==============================] - 121s 627ms/step - loss: 0.9801 - acc: 0.6282 - val_loss: 0.9270 - val_acc: 0.6436\n",
      "Epoch 3/10\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.7583 - acc: 0.7195\n",
      "Epoch 00003: val_loss improved from 0.92700 to 0.82089, saving model to best.h5\n",
      "193/193 [==============================] - 125s 648ms/step - loss: 0.7583 - acc: 0.7195 - val_loss: 0.8209 - val_acc: 0.6859\n",
      "Epoch 4/10\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.5984 - acc: 0.7865\n",
      "Epoch 00004: val_loss improved from 0.82089 to 0.75393, saving model to best.h5\n",
      "193/193 [==============================] - 130s 674ms/step - loss: 0.5984 - acc: 0.7865 - val_loss: 0.7539 - val_acc: 0.7205\n",
      "Epoch 5/10\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.4954 - acc: 0.8272"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "# 단어의미 이해 : 텍스트에선 embbeding 층\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(tk.word_index)+1, 10, input_length = len(train_pad[0]))) \n",
    "# model.add(Embedding(len(tk.word_index)+1, 300, input_length = 52, trainable = False, weights = [embedding_matrix])) #trainable = False 임베딩층안쓰고 가져온 것 쓰기 \n",
    "\n",
    "#model.add(Flatten())\n",
    "\n",
    "#model.add(SimpleRNN(32)) # RNN모델\n",
    "model.add(LSTM(128))       # LSTN모델, 문맥적 정보 파악\n",
    "\n",
    "model.add(Dense(5,activation = \"softmax\"))\n",
    "\n",
    "model.compile(metrics = [\"acc\"], loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\")\n",
    "\n",
    "es = EarlyStopping(patience = 3, verbose = 1)\n",
    "mc = ModelCheckpoint(\"best.h5\", save_best_only = True, verbose = 1)\n",
    "\n",
    "model.fit(train_pad, train[\"author\"], batch_size = 256, validation_split = 0.1, epochs = 10, callbacks = [es,mc])\n",
    "\n",
    "result = model.predict(test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = result.argmax(1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  0  1  2  3  4\n",
       "0      0  0  0  0  0  0\n",
       "1      1  0  0  0  0  0\n",
       "2      2  0  0  0  0  0\n",
       "3      3  0  0  0  0  0\n",
       "4      4  0  0  0  0  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[['0','1','2','3','4']] = result\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"sub.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
